# ğŸ—‚ï¸ **PMU Prediction Project â€” Planning & Status**

## ğŸ“Œ **Project Goal**

Build a full horse-race prediction system using PMU API data:

* Automatic data ingestion from PMU JSON endpoints
* Normalized PostgreSQL database (Supabase)
* Daily automated ingestion
* ML training pipeline
* Prediction API + deployment
* Optional front-end / dashboard

---

# âœ… **1. Architecture & Setup**

### âœ”ï¸ Status: **COMPLETED**

* Defined global architecture (ingestion â†’ database â†’ ML â†’ API)
* Created project structure (`src/`, `scripts/`, `tests/`, etc.)
* Defined full SQL schema (10 tables)
* Connected DBeaver to Supabase
* Solved IPv6 / Supabase connection issue using **pooler IPv4**
* Added `.env`, `.env.example`, `.gitignore`, requirements.txt
* Implemented repo-dump script (`dump_repo.py`)

### ğŸ”œ Nothing left here.

---

# ğŸ“ **2. Database Schema Creation**

### âœ”ï¸ Status: **COMPLETED**

* Implemented all SQL tables:

  * `daily_program`
  * `race_meeting`
  * `race`
  * `horse`
  * `race_participant`
  * `horse_race_history`
  * `race_bet`
  * `bet_report`
  * `prediction`
  * raw tables (JSON backups)
* Added missing column `race_status_category`
* Verified schema integrity via DBeaver

### ğŸ”œ Nothing left here.

---

# ğŸ“¡ **3. JSON Inspection (Quality & Feature Coverage)**

### âœ”ï¸ Status: **COMPLETED**

* Created 4 inspection scripts:

  * `inspect_programme.py`
  * `inspect_participants.py`
  * `inspect_performances.py`
  * `inspect_rapports.py`
* Verified expected fields vs API actual fields (JSON 1â†’4)
* Created Markdown **feature reliability** file in canvas

### ğŸ”œ Nothing left here.

---

# ğŸ“¥ **4. JSON 1 Ingestion (Programme du Jour)**

### âœ”ï¸ Status: **COMPLETED**

Scripts:

* `ingest_programme_day.py`

Tasks done:

* Ingests into:

  * `daily_program`
  * `race_meeting`
  * `race`
* Handling:

  * timestamps â†’ dates
  * duration (ms â†’ seconds)
  * penetrometer with French decimal `"4,2"` fixed
  * upsert logic
  * logging

### ğŸ”µ Where you stopped

JSON 1 ingestion now **works end-to-end**.

### ğŸ”œ Next:

â¡ï¸ Move to ingestion of **JSON 2** (Participants)

---

# ğŸ **5. JSON 2 Ingestion â€” Participants & Horses**

### â³ Status: **NEXT STEP**

This will fill:

* `horse`
* `race_participant`

Tasks to do:

* Fetch JSON 2 per course:

  ```
  /rest/client/61/programme/{date}/R{numReunion}/C{numCourse}/participants
  ```
* Upsert horses (avoid duplicates with same name)
* Insert/update race participants:

  * age, sexe, trainer, driver
  * gains, musique, reports (ref odds, live odds)
  * post-race leakage fields stored too
* Link to the right `race_id` using programme table

### ğŸ”œ To be implemented next (we can do it together)

---

# ğŸ“š **6. JSON 3 Ingestion â€” Performances dÃ©taillÃ©es**

### â³ Status: **PENDING**

Will populate:

* `horse_race_history`

Tasks:

* Convert dates, allocations, distances
* Handle conditional fields (many nulls)
* Ensure correct identification of â€œitsHimâ€

---

# ğŸ’¸ **7. JSON 4 Ingestion â€” Rapports dÃ©finitifs**

### â³ Status: **PENDING**

Will fill:

* `race_bet`
* `bet_report`

Tasks:

* Insert one row per pari
* Insert many rows per rapport (dividendes)
* Handle `rembourse` (bool)

---

# ğŸ¤– **8. Feature Engineering & ML Pipeline**

### â³ Status: **PENDING**

Tasks:

* Build feature table or on-the-fly features:

  * horse speed metrics
  * form indicators
  * trainer stats
  * race difficulty
  * bet odds transformations
* Split train / val sets
* Select model:

  * XGBoost
  * LightGBM
  * Logistic regression baseline
* Train, evaluate, log metrics
* Save model weights & version

---

# ğŸŒ **9. Prediction API (FastAPI)**

### â³ Status: **PENDING**

Tasks:

* Create FastAPI app
* Endpoints:

  * `/predict/today`
  * `/predict/race/{race_id}`
  * `/health`
* Load latest model
* Generate predictions per horse
* Store predictions in DB

Deployment options:

* Railway
* Render
* Supabase Function (experimental)
* VPS cheap option

---

# ğŸ” **10. Automation & Scheduling**

### â³ Status: **PENDING**

* GitHub Actions or cron job:

  * Every morning: ingest JSON 1â†’4
  * After ingestion: run prediction script
  * Insert predictions into DB
* Monitor failures via logging

---

# ğŸ“Š **11. Optional: Front-End**

### â³ Status: **OPTIONAL**

* Streamlit dashboard
* Simple web UI (React or plain HTML)
* Display predictions & past accuracy

---

# ğŸ§© **Current Position in the Project**

### âœ”ï¸ You have completed:

* Architecture
* Database schema
* JSON analysis
* JSON 1 ingestion working end-to-end

### ğŸ”¥ **Next concrete step:**

ğŸ‘‰ Implement **JSON 2 ingestion** (participants & horses)

---

