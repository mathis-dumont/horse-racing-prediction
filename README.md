# PrÃ©diction des courses hippiques par machine learning

Ce projet vise Ã  concevoir une plateforme complÃ¨te de traitement de donnÃ©es et de prÃ©diction des courses hippiques (Trot), basÃ©e sur des techniques de Machine Learning (XGBoost).

Il couvre lâ€™ensemble de la chaÃ®ne :
- ingestion automatisÃ©e des donnÃ©es (ETL),
- stockage et historisation (PostgreSQL),
- entraÃ®nement et dÃ©ploiement dâ€™un modÃ¨le prÃ©dictif,
- exposition des rÃ©sultats via une API REST et une interface utilisateur.

---

## Objectifs du projet

* Centraliser et historiser les donnÃ©es PMU (programmes, partants, performances, rapports)
* GÃ©nÃ©rer des **probabilitÃ©s calibrÃ©es de victoire**
* DÃ©tecter des **edges mathÃ©matiques exploitables**
* Fournir une **UI claire et rapide** pour lâ€™analyse quotidienne
* Assurer une **exÃ©cution automatisÃ©e et reproductible** (CI/CD & CRON)

---

## Installation & DÃ©marrage

Ce projet est conÃ§u pour Ãªtre lancÃ© rapidement via **Docker** (recommandÃ©). Une installation locale manuelle est Ã©galement possible pour le dÃ©veloppement spÃ©cifique.

### PrÃ©requis

Avant de commencer, assurez-vous d'avoir installÃ© les outils suivants sur votre machine :

*   **Docker** & **Docker Compose** (Indispensable pour la mÃ©thode recommandÃ©e).
*   **Make** (Pour utiliser les raccourcis d'automatisation).
Parfait ğŸ‘
Voici **lâ€™intÃ©gration propre** de lâ€™installation de `make` **directement dans ton README**, au **bon endroit**, avec un **ton acadÃ©mique clair**.

Tu peux **remplacer ta section â€œPrÃ©requisâ€ actuelle** par celle-ci.

---

Voici la version **mise Ã  jour et bien structurÃ©e** de ta section **PrÃ©requis**, avec le Makefile intÃ©grÃ© et un ton clair pour un projet de cours :

---

## PrÃ©requis

Avant de commencer, assurez-vous dâ€™avoir installÃ© les outils suivants sur votre machine :

### Docker

* **Docker** & **Docker Compose**
  Indispensable pour la mÃ©thode recommandÃ©e (exÃ©cution via conteneurs).

---

### Make

Le projet utilise un **Makefile** comme point dâ€™entrÃ©e unique pour :

* lancer les services Docker,
* exÃ©cuter les tests,
* dÃ©clencher les pipelines ETL et Machine Learning.

Lâ€™outil `make` doit donc Ãªtre installÃ© sur votre machine.

#### Linux (Ubuntu / Debian)

```bash
sudo apt update
sudo apt install make
```

#### macOS

Installez les outils de dÃ©veloppement Apple (inclut `make`) :

```bash
xcode-select --install
```

#### Windows

Deux solutions sont possibles :

**Option 1 â€“ WSL (recommandÃ©)**
Installez Ubuntu via le Microsoft Store, puis :

```bash
sudo apt update
sudo apt install make
```

**Option 2 â€“ Git Bash**
Installez *Git for Windows*, puis vÃ©rifiez que `make` est disponible :

```bash
make --version
```

---

### Python et base de donnÃ©es (installation locale uniquement)

Ces prÃ©requis sont nÃ©cessaires **uniquement si vous nâ€™utilisez pas Docker** :

* **Python 3.12+**
* **PostgreSQL**

---

### Configuration

1.  **Clonage du projet :**
    ```bash
    git clone <url-du-repo>
    cd <nom-du-projet>
    ```

2.  **Variables d'environnement :**
    Le projet nÃ©cessite un fichier `.env` Ã  la racine pour fonctionner (connexion BDD)
    
    CrÃ©ez un fichier `.env` Ã  la racine et renseignez les variables suivantes :
    ```ini
    # Exemple de configuration .env
    DB_URL=postgresql://user:password@host.docker.internal:5432/nom_de_la_bdd
    ```
    > **Note :** Si le mot de passe de la base de donnÃ©es supabase ne vous a pas Ã©tÃ© fourni, vous devez faire tourner votre base de donnÃ©es PostgreSQL sur votre machine hÃ´te, utilisez `host.docker.internal` comme hÃ´te dans `DB_URL` pour que le conteneur Docker puisse l'atteindre.

---

### MÃ©thode 1 : DÃ©marrage rapide avec Docker (RecommandÃ©)

L'utilisation du `Makefile` simplifie grandement l'interaction avec Docker Compose.

**1. Construction des images**
Compilez les images Docker pour le backend et le frontend.
```bash
make build
# Ou pour forcer une reconstruction sans cache : make build-nc
```

**2. Lancement des services**
DÃ©marre les conteneurs en arriÃ¨re-plan (mode dÃ©tachÃ©).
```bash
make up
```

**3. Initialisation des donnÃ©es (ETL & ML)**
Une fois les conteneurs lancÃ©s, vous devez peupler la base de donnÃ©es et entraÃ®ner le modÃ¨le.

*   **EntraÃ®nement du modÃ¨le (Training) :**
    EntraÃ®ne le modÃ¨le XGBoost sur les donnÃ©es prÃ©sentes en base.
    ```bash
    make train
    ```

*   **Optionnel :** Ingestion des donnÃ©es (ETL) :
    TÃ©lÃ©charge et insÃ¨re les programmes, participants et rapports pour la date du jour.
    ```bash
    make ingest
    ```
    *Ceci est optionnel, car la base de donnÃ©es est hÃ©bergÃ©e sur supabase, et est peuplÃ©e tous les jours par un CRON."

**4. AccÃ¨s Ã  l'application**
*   **Frontend (Interface Utilisateur) :** [http://localhost:8501](http://localhost:8501)
*   **Backend (API Documentation) :** [http://localhost:8000/docs](http://localhost:8000/docs)

**Commandes utiles :**
*   `make logs` : Affiche les logs en temps rÃ©el.
*   `make down` : ArrÃªte les conteneurs.
*   `make clean` : ArrÃªt complet, suppression des volumes et nettoyage des caches Python (`__pycache__`).

---

### MÃ©thode 2 : Installation et lancement en Local (Sans Docker)

Si vous devez dÃ©velopper sans Docker, suivez ces Ã©tapes. Vous aurez besoin de deux terminaux.

#### PrÃ©requis spÃ©cifiques
*   Assurez-vous que votre la base de donnÃ©es PostgreSQL est accessible dans le .env. (cf configuration partie Docker).

#### 1. Backend (API)

Dans un **premier terminal** :

```bash
cd backend

# CrÃ©ation et activation de l'environnement virtuel
python3.12 -m venv .venv
source .venv/bin/activate  # Sur Windows: .venv\Scripts\activate

# Installation des dÃ©pendances
pip install -r requirements.txt

# Configuration du PYTHONPATH (Important pour les imports absolus 'src.*')
export PYTHONPATH=$PYTHONPATH:$(pwd)

# Lancement du serveur API
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

#### 2. Frontend (Streamlit)

Dans un **second terminal** :

```bash
cd frontend

# CrÃ©ation et activation de l'environnement virtuel
python3.12 -m venv .venv
source .venv/bin/activate

# Installation des dÃ©pendances
pip install -r requirements.txt

# Configuration du PYTHONPATH
export PYTHONPATH=$PYTHONPATH:$(pwd)

# Lancement de l'application
streamlit run app.py --server.port 8501
```

---

## Automatisation & CI/CD (GitHub Actions)

Le projet intÃ¨gre une **automatisation complÃ¨te** via **GitHub Actions**, garantissant la mise Ã  jour des donnÃ©es dans la base de donnÃ©es.

### 1. Ingestion quotidienne (CRON)

Un workflow GitHub Actions est exÃ©cutÃ© **quotidiennement** pour :

* RÃ©cupÃ©rer automatiquement les donnÃ©es PMU du jour
* Mettre Ã  jour la base **Supabase (PostgreSQL)**
* Garantir une base toujours synchronisÃ©e sans intervention manuelle

Cette automatisation explique pourquoi lâ€™Ã©tape `make ingest` est optionnelle.

---

## Architecture & Fonctionnement

Ce projet repose sur une architecture **Microservices-lite**, sÃ©parant clairement responsabilitÃ©s et flux de donnÃ©es.

### Vue dâ€™ensemble

1. **Frontend (Streamlit)**

   * Dashboard interactif
   * Aucune connexion directe Ã  la base
   * Communication exclusive via API REST

2. **Backend (FastAPI)**

   * Exposition des endpoints mÃ©tier
   * Chargement du modÃ¨le ML en mÃ©moire au dÃ©marrage

3. **Data & ML Pipeline**

   * **ETL** : ingestion multithreadÃ©e des donnÃ©es PMU
   * **Training** : gÃ©nÃ©ration dâ€™un modÃ¨le XGBoost
   * **Inference** : prÃ©dictions temps rÃ©el via API

---

## Tests & QualitÃ© du Code

Le projet inclut des commandes pour exÃ©cuter les tests unitaires et d'intÃ©gration via Docker, garantissant un environnement isolÃ©.

```bash
# Lancer les tests Backend (Pytest)
make test-backend

# Lancer les tests Frontend (Pytest + Mocking)
make test-frontend

# Lancer tous les tests
make test-all
```

---

## Structure du Projet

L'organisation des fichiers suit les standards de l'industrie pour assurer maintenabilitÃ© et scalabilitÃ©.

```bash
.
â”œâ”€â”€ Makefile                # Orchestrateur des commandes (Entry point)
â”œâ”€â”€ docker-compose.yml      # Configuration des services Docker
â”œâ”€â”€ .env                    # Variables d'environnement (Non versionnÃ©)
â”‚
â”œâ”€â”€ backend/                # API & Logique ML
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/            # Routes FastAPI, Schemas (Pydantic), Repositories
â”‚   â”‚   â”œâ”€â”€ core/           # Config globale, Database Manager
â”‚   â”‚   â”œâ”€â”€ ingestion/      # ETL (Programmes, Participants, Rapports)
â”‚   â”‚   â”œâ”€â”€ ml/             # Feature Engineering, Training, Prediction
â”‚   â”‚   â””â”€â”€ cli/            # Scripts en ligne de commande (ex: etl.py)
â”‚   â””â”€â”€ tests/              # Tests unitaires et d'intÃ©gration (Pytest)
â”‚
â”œâ”€â”€ frontend/               # Interface Streamlit
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ app.py              # Point d'entrÃ©e de l'application
â”‚   â”œâ”€â”€ ui/                 # Composants visuels (Sidebar, Grids, Analysis)
â”‚   â”œâ”€â”€ state/              # Gestion d'Ã©tat de session
â”‚   â”œâ”€â”€ api/                # Client HTTP interne vers le Backend
â”‚   â””â”€â”€ tests/              # Tests End-to-End et UI
â”‚
â””â”€â”€ data/                   # modÃ¨le ML (.pkl) et dumps locaux
```

---

## FonctionnalitÃ©s Principales

### 1. Programme & Cotes

* RÃ©unions et courses par date
* Partants, drivers, entraÃ®neurs
* Cotes live

### 2. PrÃ©dictions IA

* ProbabilitÃ©s calibrÃ©es (0â€“100 %)
* Ranking prÃ©dictif
* Feature engineering (forme, musique, dÃ©ferrageâ€¦)

### 3. Module â€œSniperâ€

StratÃ©gie automatisÃ©e de **Value Betting** :

* Comparaison IA vs marchÃ©
* DÃ©tection dâ€™edges positifs
* Filtres stricts (cotes, edge min, liquiditÃ©)

---

## Bonnes pratiques & Ops

### Gestion des erreurs Docker
Si vous rencontrez des erreurs de permissions (ex: `Permission denied: '__pycache__'`) dues Ã  la crÃ©ation de fichiers par Docker (root) sur votre systÃ¨me hÃ´te, utilisez la commande :

```bash
make clean
# Cette commande arrÃªte les conteneurs et force la suppression des caches avec sudo
```

### Ajouter une nouvelle dÃ©pendance
Si vous ajoutez une librairie dans `backend/requirements.txt` ou `frontend/requirements.txt`, vous devez reconstruire les images :

```bash
make build
```

---

## Documentation API

Une fois le backend lancÃ©, la documentation interactive (Swagger UI) est disponible automatiquement. Elle permet de tester les endpoints et de voir les schÃ©mas de donnÃ©es attendus.

*   **URL Locale :** `http://localhost:8000/docs`
*   **SchÃ©ma JSON :** `http://localhost:8000/openapi.json`

---

## Contribution

1. CrÃ©ez une branche (`feature/ma-feature`)
2. ImplÃ©mentez + testez (`make test-all`)
3. Committez
4. Ouvrez une Pull Request

---